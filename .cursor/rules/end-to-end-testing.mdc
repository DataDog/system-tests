---
description: 
globs: 
alwaysApply: true
---
# End-to-End

* To run the tests associated to an end to end scenario use the document [run the tests](mdc:docs/execute/run.md).
* You can find information about the end-to-end weblogs in the document [end-to-end weblog specification](mdc:docs/weblog/README.md).
* All new weblog endpoints MUST be listed in the "Endpoints" section of the document [end-to-end weblog specification](mdc:docs/weblog/README.md).
* To know about all the end-to-end weblogs available on system-tests, list all docker files in the folder [weblog folder](mdc:utils/build/docker). This is the pattern to discover them: "utils/build/docker/<language>/<weblog_name>.Dockerfile".
* A new end-to-end weblog must be referenced in the document [build script documentation](mdc:docs/execute/build.md).
* if you create a new end to end weblog, create it always with one example endpoint.
* if you craete a new end to end weblog you always must to show a list of the end to end endpoints that are registered on system-tests, reading the section "Endpoints" of the document [end-to-end weblog specification](mdc:docs/weblog/README.md). Ask to the user if he wants to implement one or more endpoints of the list in the new weblog.
* After creating a new end to end weblog ALWAYS check the compatible scenarios on [workflow datada](mdc:utils/scripts/ci_orchestrators/workflow_data.py) using the method "_is_supported".
* The end to end scenario test cases use the "weblog" object defined in the python file [weblog object](mdc:utils/_weblog.py) to make request to the endpoints (weblog.request, weblog.get, weblog.post, etc).
* To know how to add a new end to end tests case use the document [how to add a new end to end test](mdc:docs/edit/add-new-test.md).
* The logs folder structure for the end to end scenarios is explained in the document [logs folder structure end to end scenarios](mdc:docs/execute/logs.md).
* CRITICAL BUILD COMMAND: When providing build commands for end-to-end weblogs, ALWAYS use the correct format: `./build.sh <language> -w <weblog-variant>` (e.g., `./build.sh java -w spring-boot`, `./build.sh python -w flask-poc`, `./build.sh nodejs -w express4`). NEVER use `./build.sh <weblog-variant>` without the language parameter. The language parameter is MANDATORY. The correct syntax is documented in [build script documentation](mdc:docs/execute/build.md).
* End to End scenarios store and validate messages:
  * Intercepted between instrumented application (instrumented by library/datadog tracer) and datadog agent. these messages are stored as json files in the logs folder: logs_<scenario_name>/interfaces/library. These intercepted json messages are parsed and validated using the "library interface" implemented as class "LibraryInterfaceValidator" in the [Libray interface core implementation](mdc:utils/interfaces/_library/core.py). For details use the [library interface](mdc:docs/internals/library-interface-validation-methods.md) documentation.
  * Intercepted between datadog agent and datadog backend. These messages are stored as json files in the logs folder: logs_<scenario_name>/interfaces/agent. These intercepted json messages are parsed and validated using the "agent interface" implemented as class "AgentInterfaceValidator" in the [agent interface core implementation](mdc:utils/interfaces/_agent.py). For details use the [agent interface](mdc:docs/internals/agent-interface-validation-methods.md) documentation.
  * Returned by the datadog backend API. These messages are stored as json files in the logs folder: logs_<scenario_name>/interfaces/backend/files. The "backend interface" implemented as class "_BackendInterfaceValidator" in the [backend interface core implementation](mdc:utils/interfaces/_backend.py) exposes methods to interact with the datadog backend API. For details use the [backend interface](mdc:docs/internals/backend-interface-validation-methods.md) documentation.
* The library interface, agent interface and backend interface are instantiated as singleton in [validation interfaces](mdc:utils/interfaces/__init__.py) and they can be used by the end to end test cases as following example:

```python
from utils import interfaces, weblog, scenarios

@scenarios.scenario
class my_test_class:
    def setup_mytestcase(self):
        # The system-tests framework will execute the setup method before the test method
        weblog.request("/url_that_perform_and_action_and_generate_traces_to_be_intercepted")
    
    def test_mytestcase(self):
        # The traces/messages should be generated in the setup method and intercepted by the library, agent or backend interfaces and stored in the log folder
        # Use the interfaces instances to validate the json intercepted messages
        interfaces.library.<validation method>
        interfaces.agent.<validation method>
        interfaces.backend.<validation method>
```

* The class "SchemaValidator" from [schema validator](mdc:utils/interfaces/_schemas_validators.py) can help you to validate the messages under the folder "logs_<scenario_name>/interfaces."
* if the user wants to create a new scenario, you MUST validate or ask to the user the context if this scenario is for: end to end, parametric, kubernetes or ssi.
* After adding a new scenario on [scenarios](mdc:utils/_context/_scenarios/__init__.py) NEVER verify that the scenario has been properly added.
* After adding a new end to end scenario ask to the user if needs help to include the scenario in the CI GitHub Workflow.
* Use [github end to end ci](mdc:.github/workflows/run-end-to-end.yml) and [workflow datada](mdc:utils/scripts/ci_orchestrators/workflow_data.py) to register a new end to end scenario on Github CI.
