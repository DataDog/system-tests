include:
 # - remote: https://gitlab-templates.ddbuild.io/libdatadog/include/single-step-instrumentation-tests.yml
  - remote: https://gitlab-templates.ddbuild.io/libdatadog/include-wip/robertomonteromiguel/onboarding_tests_new_aws_account/single-step-instrumentation-tests.yml

stages:
  - configure
  - nodejs
  - java
  - dotnet
  - python
  - php
  - ruby
  - pipeline-status
  - system-tests-utils

variables:
    # Do not modify this - must be the repository name for Kubernetes gitlab runners to run
    KUBERNETES_SERVICE_ACCOUNT_OVERWRITE: system-tests #helm-charts
    TEST: 1

compute_pipeline:
  image: registry.ddbuild.io/ci/libdatadog-build/system-tests:57161036
  tags: ["arch:amd64"]
  stage: configure
  variables:
    CI_ENVIRONMENT: "prod"
  script:
    - |
      if [ -z "$SYSTEM_TESTS_SCENARIOS" ] && [ -z "$SYSTEM_TESTS_SCENARIOS_GROUPS" ]; then
        echo "❌ Both SYSTEM_TESTS_SCENARIOS and SYSTEM_TESTS_SCENARIOS_GROUPS are not defined.Computing changes..."
        ./build.sh -i runner
        source venv/bin/activate
        ./run.sh MOCK_THE_TEST --collect-only --scenario-report
        git clone https://github.com/DataDog/system-tests.git original
        git fetch --all  # Ensure all branches are available
        git branch --track $CI_COMMIT_REF_NAME origin/$CI_COMMIT_REF_NAME || true  # Track branch if not tracked
        git checkout $CI_COMMIT_REF_NAME  # Ensure the branch is checked out
        BASE_COMMIT=$(git merge-base origin/main $CI_COMMIT_REF_NAME)  # Get the base commit
        echo "Branch was created from commit--> $BASE_COMMIT"
        git diff --name-only $BASE_COMMIT $CI_COMMIT_SHA  >> modified_files.txt # List modified files
        cat modified_files.txt
        python utils/scripts/compute_impacted_scenario.py >> impacted_scenarios.txt
        cat impacted_scenarios.txt
        source impacted_scenarios.txt
      else
          echo "✅ SYSTEM_TESTS_SCENARIOS OR SYSTEM_TESTS_SCENARIOS_GROUPS are set."
          export scenarios=$SYSTEM_TESTS_SCENARIOS
          export scenarios_groups=$SYSTEM_TESTS_SCENARIOS_GROUPS
          cd /system-tests
          git pull # update to most recent system-tests
          ./build.sh -i runner
          source venv/bin/activate
      fi
    - python utils/scripts/compute-workflow-parameters.py nodejs -s "$scenarios" -g "$scenarios_groups" --parametric-job-count 1 --ci-environment "${CI_ENVIRONMENT}" --format gitlab
    - python utils/scripts/compute-workflow-parameters.py java -s "$scenarios" -g "$scenarios_groups" --parametric-job-count 1 --ci-environment "${CI_ENVIRONMENT}" --format gitlab
    - python utils/scripts/compute-workflow-parameters.py dotnet -s "$scenarios" -g "$scenarios_groups" --parametric-job-count 1 --ci-environment "${CI_ENVIRONMENT}" --format gitlab
    - python utils/scripts/compute-workflow-parameters.py python -s "$scenarios" -g "$scenarios_groups" --parametric-job-count 1 --ci-environment "${CI_ENVIRONMENT}" --format gitlab
    - python utils/scripts/compute-workflow-parameters.py php -s "$scenarios" -g "$scenarios_groups" --parametric-job-count 1 --ci-environment "${CI_ENVIRONMENT}" --format gitlab
    - python utils/scripts/compute-workflow-parameters.py ruby -s "$scenarios" -g "$scenarios_groups" --parametric-job-count 1 --ci-environment "${CI_ENVIRONMENT}" --format gitlab
    - |
      if [ -n "$SYSTEM_TESTS_SCENARIOS" ] || [ -n "$SYSTEM_TESTS_SCENARIOS_GROUPS" ]; then
        cp *.yml "$CI_PROJECT_DIR"
      fi
  rules:
    - if: '$SCHEDULED_JOB == ""'
    - if: '$SCHEDULED_JOB == null'
  artifacts:
    paths:
      - nodejs_ssi_gitlab_pipeline.yml
      - java_ssi_gitlab_pipeline.yml
      - dotnet_ssi_gitlab_pipeline.yml
      - python_ssi_gitlab_pipeline.yml
      - php_ssi_gitlab_pipeline.yml
      - ruby_ssi_gitlab_pipeline.yml

nodejs_ssi_pipeline:
  stage: nodejs
  needs: ["compute_pipeline"]
  allow_failure: true
  variables:
    PARENT_PIPELINE_SOURCE: $CI_PIPELINE_SOURCE
  trigger:
    include:
      - artifact: nodejs_ssi_gitlab_pipeline.yml
        job: compute_pipeline
    strategy: depend
  rules:
    - if: '$SCHEDULED_JOB == ""'
    - if: '$SCHEDULED_JOB == null'

java_ssi_pipeline:
  stage: java
  needs: ["compute_pipeline", "nodejs_ssi_pipeline"]
  allow_failure: true
  variables:
    PARENT_PIPELINE_SOURCE: $CI_PIPELINE_SOURCE
  trigger:
    include:
      - artifact: java_ssi_gitlab_pipeline.yml
        job: compute_pipeline
    strategy: depend
  rules:
    - if: '$SCHEDULED_JOB == ""'
    - if: '$SCHEDULED_JOB == null'

dotnet_ssi_pipeline:
  stage: dotnet
  needs: ["compute_pipeline", "java_ssi_pipeline"]
  allow_failure: true
  variables:
    PARENT_PIPELINE_SOURCE: $CI_PIPELINE_SOURCE
  trigger:
    include:
      - artifact: dotnet_ssi_gitlab_pipeline.yml
        job: compute_pipeline
    strategy: depend
  rules:
    - if: '$SCHEDULED_JOB == ""'
    - if: '$SCHEDULED_JOB == null'

python_ssi_pipeline:
  stage: python
  needs: ["compute_pipeline", "dotnet_ssi_pipeline"]
  allow_failure: true
  variables:
    PARENT_PIPELINE_SOURCE: $CI_PIPELINE_SOURCE
  trigger:
    include:
      - artifact: python_ssi_gitlab_pipeline.yml
        job: compute_pipeline
    strategy: depend
  rules:
    - if: '$SCHEDULED_JOB == ""'
    - if: '$SCHEDULED_JOB == null'

php_ssi_pipeline:
  stage: php
  needs: ["compute_pipeline", "python_ssi_pipeline"]
  allow_failure: true
  variables:
    PARENT_PIPELINE_SOURCE: $CI_PIPELINE_SOURCE
  trigger:
    include:
      - artifact: php_ssi_gitlab_pipeline.yml
        job: compute_pipeline
    strategy: depend
  rules:
    - if: '$SCHEDULED_JOB == ""'
    - if: '$SCHEDULED_JOB == null'

ruby_ssi_pipeline:
  stage: ruby
  needs: ["compute_pipeline", "php_ssi_pipeline"]
  allow_failure: true
  variables:
    PARENT_PIPELINE_SOURCE: $CI_PIPELINE_SOURCE
  trigger:
    include:
      - artifact: ruby_ssi_gitlab_pipeline.yml
        job: compute_pipeline
    strategy: depend
  rules:
    - if: '$SCHEDULED_JOB == ""'
    - if: '$SCHEDULED_JOB == null'

check_pipeline_status:
  stage: pipeline-status
  image: registry.ddbuild.io/images/ci_docker_base
  tags: ["runner:docker"]
  script:
    - echo "Checking for failed jobs in parent and child pipelines..."
    - export GITLAB_TOKEN=$(aws ssm get-parameter --region us-east-1 --name "ci.$CI_PROJECT_NAME.gitlab.token" --with-decryption --query "Parameter.Value" --out text)

    # Function to fetch all jobs from a paginated GitLab API endpoint
    - |
      fetch_all_jobs() {
        local url="$1"
        local failed_count=0
        local page=1

        while true; do
          RESPONSE=$(curl --silent --header "PRIVATE-TOKEN: $GITLAB_TOKEN" "$url&page=$page&per_page=100")

          # Validate JSON response
          if ! echo "$RESPONSE" | jq -e type > /dev/null 2>&1; then
            echo "❌ Error: Invalid JSON response from GitLab API."
            exit 1
          fi

          # Count failed jobs on this page
          local page_failed_jobs
          page_failed_jobs=$(echo "$RESPONSE" | jq '[.[] | select(.status == "failed")] | length')
          failed_count=$((failed_count + page_failed_jobs))

          # Check if there are more pages (by checking if we got less than 100 results)
          local jobs_count
          jobs_count=$(echo "$RESPONSE" | jq 'length')

          if [ "$jobs_count" -lt 100 ]; then
            break
          fi

          ((page++))
        done

        echo "$failed_count"
      }

    # Fetch failed jobs in the parent pipeline
    - |
      FAILED_JOBS=$(fetch_all_jobs "$CI_API_V4_URL/projects/$CI_PROJECT_ID/pipelines/$CI_PIPELINE_ID/jobs?")

      # Fetch child pipelines
      CHILD_PIPELINES=$(curl --silent --header "PRIVATE-TOKEN: $GITLAB_TOKEN" \
        "$CI_API_V4_URL/projects/$CI_PROJECT_ID/pipelines/$CI_PIPELINE_ID/bridges" | jq -r '.[].downstream_pipeline.id')

      FAILED_CHILD_JOBS=0
      for CHILD_PIPELINE in $CHILD_PIPELINES; do
        FAILED_CHILD_JOBS=$((FAILED_CHILD_JOBS + $(fetch_all_jobs "$CI_API_V4_URL/projects/$CI_PROJECT_ID/pipelines/$CHILD_PIPELINE/jobs?")))
      done

      TOTAL_FAILED=$((FAILED_JOBS + FAILED_CHILD_JOBS))

      if [ "$TOTAL_FAILED" -gt 0 ]; then
        echo "❌ Pipeline failed: $TOTAL_FAILED job(s) did not succeed."
        exit 1
      fi

    - echo "✅ All jobs and child pipelines passed!"
  when: always
  rules:
    - if: '$SCHEDULED_JOB == ""'
    - if: '$SCHEDULED_JOB == null'

delete_amis:
  extends: .base_job_onboarding
  stage: system-tests-utils
  allow_failure: false
  variables:
    AMI_RETENTION_DAYS: 10
    AMI_LAST_LAUNCHED_DAYS: 10
  script:
        - ./build.sh -i runner
        - source venv/bin/activate
        - python utils/scripts/pulumi_clean_up.py --component amis --ami-retention-days $AMI_RETENTION_DAYS --ami-last-launched-days $AMI_LAST_LAUNCHED_DAYS
  rules:
    - if: '$SCHEDULED_JOB == "delete_amis"'
  after_script: echo "Finish"
  timeout: 3h

delete_amis_by_name_or_lang:
  extends: .base_job_onboarding
  stage: system-tests-utils
  allow_failure: false
  script:
        - |
          if [ -z "$AMI_NAME" ] && [ -z "$AMI_LANG" ]; then
            echo "❌ ERROR: Either AMI_NAME or AMI_LANG must be set."
            exit 1
          fi
          echo "✅ Proceeding with AMI_NAME=$AMI_NAME, AMI_LANG=$AMI_LANG"
        - ./build.sh -i runner
        - source venv/bin/activate
        - |
          CMD="python utils/scripts/pulumi_clean_up.py --component amis_by_name"

          if [ -n "$AMI_NAME" ]; then
            CMD="$CMD --ami-name $AMI_NAME"
          fi

          if [ -n "$AMI_LANG" ]; then
            CMD="$CMD --ami-lang $AMI_LANG"
          fi

          echo "Running: $CMD"
          eval $CMD
  rules:
    - if: '$SCHEDULED_JOB == "delete_amis_by_name_or_lang"'
  after_script: echo "Finish"

delete_ec2_instances:
  extends: .base_job_onboarding
  stage: system-tests-utils
  allow_failure: false
  variables:
    EC2_AGE_MINUTES: 45
  script:
        - ./build.sh -i runner
        - source venv/bin/activate
        - python utils/scripts/pulumi_clean_up.py --component ec2 --ec2-age-minutes $EC2_AGE_MINUTES
  rules:
    - if: '$SCHEDULED_JOB == "delete_ec2_instances"'
  after_script: echo "Finish"

count_amis:
  extends: .base_job_onboarding
  stage: system-tests-utils
  allow_failure: false
  script:
        - ./build.sh -i runner
        - source venv/bin/activate
        - python utils/scripts/pulumi_clean_up.py --component amis_count
  rules:
    - if: '$SCHEDULED_JOB == "count_amis"'
  after_script: echo "Finish"

check_merge_labels:
  #Build docker images if it's needed. Check if the PR has the labels associated with the image build.
  image: registry.ddbuild.io/images/ci_docker_base
  tags: ["runner:docker"]

  stage: system-tests-utils
  allow_failure: true
  before_script:
    - export GH_TOKEN=$(aws ssm get-parameter --region us-east-1 --name ci.system-tests.gh-token --with-decryption --query "Parameter.Value" --out text)
    - export DOCKER_LOGIN=$(aws ssm get-parameter --region us-east-1 --name ci.system-tests.docker-login-write --with-decryption --query "Parameter.Value" --out text)
    - export DOCKER_LOGIN_PASS=$(aws ssm get-parameter --region us-east-1 --name ci.system-tests.docker-login-pass-write --with-decryption --query "Parameter.Value" --out text)
  script:
    -  echo $GH_TOKEN | docker login ghcr.io -u publisher --password-stdin
    - ./utils/scripts/get_pr_merged_labels.sh
  rules:
    - if: $CI_PIPELINE_SOURCE == "push" && $CI_COMMIT_BRANCH == "main"

generate_system_tests_images:
  image: registry.ddbuild.io/images/ci_docker_base
  tags: ["runner:docker"]
  needs: []
  stage: system-tests-utils
  allow_failure: true
  before_script:
    - export GH_TOKEN=$(aws ssm get-parameter --region us-east-1 --name ci.system-tests.gh-token --with-decryption --query "Parameter.Value" --out text)
    - export DOCKER_LOGIN=$(aws ssm get-parameter --region us-east-1 --name ci.system-tests.docker-login-write --with-decryption --query "Parameter.Value" --out text)
    - export DOCKER_LOGIN_PASS=$(aws ssm get-parameter --region us-east-1 --name ci.system-tests.docker-login-pass-write --with-decryption --query "Parameter.Value" --out text)
  script:
      -  echo $GH_TOKEN | docker login ghcr.io -u publisher --password-stdin
      - ./utils/build/build_tracer_buddies.sh --push
      - ./utils/build/build_python_base_images.sh --push
      - ./lib-injection/build/build_lib_injection_images.sh
  when: manual

.delayed_base_job:
    image: registry.ddbuild.io/images/ci_docker_base
    tags: ["arch:amd64"]
    script:
        - echo "⏳ Waiting before triggering the child pipeline..."
    when: delayed
    start_in: 5 minutes