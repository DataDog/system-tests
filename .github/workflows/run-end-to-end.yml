name: End-to-end tests

on:
  workflow_call:
    inputs:
      runs_on:
        description: "Github runner"
        required: true
        type: string
      library:
        description: "Library to test"
        required: true
        type: string
      ref:
        description: "system-tests ref to run the tests on (can be any valid branch, tag or SHA in system-tests repo)"
        type: string
      scenarios:
        description: "JSON array of scenarios to run"
        default: "[]"
        required: false
        type: string
      weblog:
        description: "Weblog name"
        required: true
        type: string
      weblog_build_required:
        description: "Is ./build.sh required?"
        required: true
        type: boolean
      weblog_instance:
        description: "Weblog instance name, if several job runs the same weblog, usefull to distinguish them in UI"
        required: false
        type: string
        default: ''
      force_execute:
        description: "List of nodeid to force execute, separated by comma"
        required: false
        type: string
        default: ''
      binaries_artifact:
        description: "Artifact name containing the binaries to test"
        default: ''
        required: false
        type: string
      ci_environment:
        description: "Which CI environment is running the tests, used for FPD"
        default: 'custom'
        required: false
        type: string
      _build_python_base_images:
        description: "Shall we build python base images for tests on python tracer"
        default: false
        required: false
        type: boolean
      _build_buddies_images:
        description: "Shall we build buddies images"
        default: false
        required: false
        type: boolean
      _build_proxy_image:
        description: "Shall we build proxy image"
        default: false
        required: false
        type: boolean
      _build_lambda_proxy_image:
        description: "Shall we build the lambda-proxy image"
        default: false
        required: false
        type: boolean
      push_to_feature_parity_dashbaord:
        description: "Shall we push results to Feature Parity Dashbaord"
        default: false
        required: false
        type: boolean
      skip_empty_scenarios:
        description: "Skip scenarios that contains only xfail or irrelevant tests"
        default: false
        required: false
        type: boolean
      _enable_replay_scenarios:
        description: "Enable replay scenarios, should only used in system-tests CI"
        default: false
        required: false
        type: boolean
      logs_artifact_name:
        description: "The name of the artifact to use for logs"
        required: true
        type: string
      artifact_retention_days:
        description: "Maximum retention of artifacts generated by upload-artifact action"
        default: 14
        required: false
        type: number
      push_to_test_optimization:
        description: "Push test result to DataDog Test Optimization"
        default: false
        required: false
        type: boolean
      test_optimization_datadog_site:
        description: "DataDog site to use for test optimization"
        default: "datadoghq.com"
        required: false
        type: string

jobs:
  main:
    name: "${{ inputs.weblog }} ${{ inputs.weblog_instance }}"
    if: inputs.library == 'nodejs'
    runs-on: ${{ inputs.runs_on }}
    env:
      SYSTEM_TESTS_REPORT_ENVIRONMENT: ${{ inputs.ci_environment }}
      SYSTEM_TESTS_REPORT_RUN_URL: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
      SYSTEM_TESTS_SKIP_EMPTY_SCENARIO: ${{ inputs.skip_empty_scenarios }}
      SYSTEM_TESTS_FORCE_EXECUTE: ${{ inputs.force_execute }}
    steps:
    - name: Compute ref
      id: compute_ref
      run: |
        if [[ "${{ inputs.ref }}" != "" ]]; then
          echo "ref=${{ inputs.ref }}" >> $GITHUB_OUTPUT
        elif [[ "${{ github.repository }}" == "DataDog/system-tests" ]]; then
          echo "ref=" >> $GITHUB_OUTPUT
        else
          echo "ref=main" >> $GITHUB_OUTPUT
        fi
    - name: Checkout
      uses: actions/checkout@08eba0b27e820071cde6df949e0beb9ba4906955 # v4.3.0
      with:
        repository: 'DataDog/system-tests'
        ref: ${{ steps.compute_ref.outputs.ref }}
    - name: Install runner
      uses: ./.github/actions/install_runner
    - name: Get binaries artifact
      if : ${{ inputs.binaries_artifact != '' }}
      uses: actions/download-artifact@d3f86a106a0bac45b974a628896c90dbdf5c8093 # v4.3.0
      with:
        name: ${{ inputs.binaries_artifact }}
        path: binaries/
    - name: Build python's weblog base images
      if: inputs.library == 'python' && inputs._build_python_base_images
      run: |
        ./utils/build/build_python_base_images.sh
    - name: Build buddies weblog images
      if: inputs._build_buddies_images
      run: ./utils/build/build_tracer_buddies.sh
    - name: Build proxy image
      if: inputs._build_proxy_image
      run: ./build.sh -i proxy
    - name: Build lambda-proxy image
      if: inputs._build_lambda_proxy_image
      run: ./build.sh -i lambda-proxy
    - name: Pull images
      uses: ./.github/actions/pull_images
      with:
        library: ${{ inputs.library }}
        weblog: ${{ inputs.weblog }}
        scenarios: ${{ inputs.scenarios }}
        dockerhub_username: ${{ secrets.DOCKERHUB_USERNAME }}
        dockerhub_token: ${{ secrets.DOCKERHUB_TOKEN }}
    - name: Build weblog
      id: build
      run: |
        # If not required, skip but succeed so outcome == 'success'
        if [[ "${{ inputs.weblog_build_required }}" == 'false' ]]; then
          echo "Skipping build (weblog_build_required=false)"
          exit 0
        fi
        ./build.sh ${{ inputs.library }} -i weblog -w ${{ inputs.weblog }}
      env:
        SYSTEM_TEST_BUILD_ATTEMPTS: 3

    - name: Run DEBUGGER_INPRODUCT_ENABLEMENT scenario (10 iterations)
      if: always() && steps.build.outcome == 'success' && contains(inputs.scenarios, '"DEBUGGER_INPRODUCT_ENABLEMENT"')
      run: |
        FAILED_ITERATIONS=0
        TOTAL_ITERATIONS=10

        for i in $(seq 1 $TOTAL_ITERATIONS); do
          echo "=========================================="
          echo "Running iteration $i of $TOTAL_ITERATIONS"
          echo "=========================================="

          if ./run.sh DEBUGGER_INPRODUCT_ENABLEMENT tests/debugger/test_debugger_inproduct_enablement.py::Test_Debugger_InProduct_Enablement_Code_Origin::test_inproduct_enablement_code_origin --force-dd-trace-debug; then
            echo "✓ Iteration $i passed"
          else
            echo "✗ Iteration $i failed"
            FAILED_ITERATIONS=$((FAILED_ITERATIONS + 1))
            # Rename logs folder to preserve failed iteration logs
            if [ -d "logs_debugger_inproduct_enablement" ]; then
              mv logs_debugger_inproduct_enablement "logs_debugger_inproduct_enablement_failed_iter_$i"
            fi
            echo "=========================================="
            echo "Test failed on iteration $i. Stopping early."
            echo "=========================================="
            exit 1
          fi

          echo ""
        done

        echo "=========================================="
        echo "All $TOTAL_ITERATIONS iterations passed!"
        echo "=========================================="

    - name: Compress logs
      id: compress_logs
      if: always() && steps.build.outcome == 'success'
      run: |
        if compgen -G "logs*/" > /dev/null; then
          tar -czvf artifact.tar.gz logs*/
        else
          echo "No logs*/ found; skipping tar."
        fi
    - name: Upload artifact
      if: always() && steps.compress_logs.outcome == 'success'
      uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
      with:
        # log name convention to respect : logs_$SCENARIO-FAMILY_$LIBRARY_$WEBLOG_$CI-ENVIRONMENT
        name: ${{ inputs.logs_artifact_name }}
        path: artifact.tar.gz
        retention-days: ${{ inputs.artifact_retention_days }}

    - name: Push results to Feature Parity Dashboard
      if: ${{ inputs.push_to_feature_parity_dashbaord && false }} # disabled for now
      run: |
        for folder in logs*/ ; do
          curl -X POST ${FP_IMPORT_URL} \
            --fail
            --header "Content-Type: application/json" \
            --header "FP_API_KEY: ${FP_API_KEY}" \
            --data "@./${folder}feature_parity.json" \
            --include
        done
      env:
        FP_API_KEY: ${{ secrets.FP_API_KEY }}
        FP_IMPORT_URL: ${{ secrets.FP_IMPORT_URL }}

    - name: Push results to Test Optimization
      if: always() && inputs.push_to_test_optimization
      uses: ./.github/actions/push_to_test_optim
      with:
        datadog_api_key: ${{ secrets.TEST_OPTIMIZATION_API_KEY }}
        datadog_site: ${{ inputs.test_optimization_datadog_site }}
