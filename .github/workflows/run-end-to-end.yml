name: End-to-end tests

on:
  workflow_call:
    inputs:
      run_all:
        description: "Shall we run all scenarios"
        default: false
        required: false
        type: boolean
      run_replay:
        description: "Shall we run all scenarios in replay mode"
        default: false
        required: false
        type: boolean
      run_open_telemetry:
        description: "Shall we run open-telemetry scenarios"
        default: false
        required: false
        type: boolean
      run_sampling:
        description: "Shall we run sampling scenarios"
        default: false
        required: false
        type: boolean
      run_profiling:
        description: "Shall we run profiling scenario"
        default: false
        required: false
        type: boolean
      run_debugger:
        description: "Shall we run debugger scenarios"
        default: false
        required: false
        type: boolean
      run_appsec:
        description: "Shall we run AppSec scenarios"
        default: false
        required: false
        type: boolean
      run_integration:
        description: "Shall we run Integrations scenarios"
        default: false
        required: false
        type: boolean
      build_python_base_images:
        description: "Shall we build python base images for tests on python tracer"
        default: false
        required: false
        type: boolean
      build_buddies_images:
        description: "Shall we build buddies images"
        default: false
        required: false
        type: boolean
      build_proxy_image:
        description: "Shall we build proxy image"
        default: false
        required: false
        type: boolean
      push_to_feature_parity_dashbaord:
        description: "Shall we push results to Feature Parity Dashbaord"
        default: false
        required: false
        type: boolean
      library:
        description: "Library to test"
        required: true
        type: string

env:
  REGISTRY: ghcr.io

jobs:

  compute-matrix:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.compute-matrix.outputs.matrix }}
    steps:
    - name: Compute matrix
      id: compute-matrix
      shell: python
      run: |
        import json
        import os

        weblogs={
          "cpp": ["nginx"],
          "dotnet": ["poc", "uds"],
          "golang": ["chi", "echo", "gin", "net-http", "uds-echo"],
          "java": [
            "akka-http", 
            "jersey-grizzly2", 
            "play",
            "ratpack", 
            "resteasy-netty3", 
            "spring-boot-jetty", 
            "spring-boot", 
            "spring-boot-3-native", 
            "spring-boot-openliberty", 
            "spring-boot-wildfly", 
            "spring-boot-undertow", 
            "spring-boot-payara", 
            "vertx3", 
            "vertx4",
            "uds-spring-boot", 
          ],
          "nodejs": ["express4", "uds-express4", "express4-typescript", "nextjs"],
          "php": [
            *[f"apache-mod-{v}" for v in ["7.0", "7.1", "7.2", "7.3", "7.4", "8.0", "8.1", "8.2"]],
            *[f"apache-mod-{v}-zts" for v in ["7.0", "7.1", "7.2", "7.3", "7.4", "8.0", "8.1", "8.2"]],
            *[f"php-fpm-{v}" for v in ["7.0", "7.1", "7.2", "7.3", "7.4", "8.0", "8.1", "8.2"]]
          ],
          "python": ["flask-poc", "django-poc", "uwsgi-poc", "uds-flask", "python3.12", "fastapi"],
          "ruby": [
            "rack",
            "uds-sinatra",
            *[f"sinatra{v}" for v in ["14", "20", "21", "22", "30", "31", "32", "40"]],
            *[f"rails{v}" for v in ["32", "40", "41", "42", "50", "51", "52", "60", "61", "70", "71"]]
          ]
        }

        result = weblogs["${{ inputs.library }}"]

        with open(os.environ['GITHUB_OUTPUT'], 'a') as fh:
            print(f'matrix={json.dumps(result)}', file=fh)

        print(json.dumps(result, indent=2))

  end-to-end:
    runs-on: ubuntu-latest
    needs: 
      - compute-matrix
    strategy:
      matrix:
        weblog: ${{ fromJson(needs.compute-matrix.outputs.matrix) }}
        version:
        - prod
        - dev
      fail-fast: false
    env:
      TEST_LIBRARY: ${{ inputs.library }}
      WEBLOG_VARIANT: ${{ matrix.weblog }}
      SYSTEM_TESTS_REPORT_ENVIRONMENT: ${{ matrix.version }}
      SYSTEM_TESTS_REPORT_RUN_URL: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
    steps:
    - name: Checkout
      uses: actions/checkout@v4
      with:
        repository: 'DataDog/system-tests'
    - name: Install runner
      uses: ./.github/actions/install_runner
    - name: Pull images
      uses: ./.github/actions/pull_images
      with:
        pull-all: ${{ inputs.run_all || inputs.run_integration }}
    - name: Build python's weblog base images
      if: inputs.library == 'python' && inputs.build_python_base_images
      run: |
        docker buildx build --load --progress=plain -f utils/build/docker/python/fastapi.base.Dockerfile -t datadog/system-tests:fastapi.base-v0 .
        docker buildx build --load --progress=plain -f utils/build/docker/python/python3.12.base.Dockerfile -t datadog/system-tests:python3.12.base-v1 .
        docker buildx build --load --progress=plain -f utils/build/docker/python/django-poc.base.Dockerfile -t datadog/system-tests:django-poc.base-v0 .
        docker buildx build --load --progress=plain -f utils/build/docker/python/flask-poc.base.Dockerfile -t datadog/system-tests:flask-poc.base-v2 .
        docker buildx build --load --progress=plain -f utils/build/docker/python/uwsgi-poc.base.Dockerfile -t datadog/system-tests:uwsgi-poc.base-v1 .
    - name: Build buddies weblog images
      if: inputs.build_buddies_images
      run: ./utils/build/build_tracer_buddies.sh
    - name: Build proxy image
      if: inputs.build_proxy_image
      run: ./build.sh -i proxy
    #### appsec-event-rules is now a private repo. The GH_TOKEN provided can't read private repos.
    #### skipping this, waiting for a proper solution
    # - name: Load WAF rules
    #   if: matrix.version == 'dev'
    #   run: ./utils/scripts/load-binary.sh waf_rule_set
    #   env:
    #     GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    - name: Load library binary
      if: ${{ matrix.version == 'dev' }}
      run: ./utils/scripts/load-binary.sh ${{ inputs.library }}
    
      # quick and dirty fix, because we can't test for now the PHP released tracer
    - name: Load PHP library binary (skipping release tests, use snapshot instead)
      if: ${{ matrix.version == 'prod' && inputs.library == 'php'}}
      run: ./utils/scripts/load-binary.sh ${{ inputs.library }}

    - name: Load agent binary
      if: ${{ matrix.version == 'dev' }}
      run: ./utils/scripts/load-binary.sh agent
    - name: Log in to the Container registry
      if: ${{ inputs.library == 'ruby' }}
      run: echo ${{ secrets.GITHUB_TOKEN }} | docker login ${{ env.REGISTRY }} -u ${{ github.actor }} --password-stdin
    - name: Build agent
      run: SYSTEM_TEST_BUILD_ATTEMPTS=3 ./build.sh -i agent
    - name: Build weblog
      id: build
      run: SYSTEM_TEST_BUILD_ATTEMPTS=3 ./build.sh -i weblog
    - name: Run DEFAULT scenario
      if: steps.build.outcome == 'success'
      run: ./run.sh DEFAULT
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run CROSSED_TRACING_LIBRARIES scenario
      if: always() && steps.build.outcome == 'success' && (inputs.run_integration || inputs.run_all)
      run: ./run.sh CROSSED_TRACING_LIBRARIES
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run PROFILING scenario
      if: always() && steps.build.outcome == 'success' && (inputs.run_profiling || inputs.run_all)
      run: ./run.sh PROFILING
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run TRACE_PROPAGATION_STYLE_W3C scenario
      if: always() && steps.build.outcome == 'success' && inputs.library != 'python' && inputs.run_all
      run: ./run.sh TRACE_PROPAGATION_STYLE_W3C
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run INTEGRATIONS scenario
      if: always() && steps.build.outcome == 'success' && (inputs.run_integration || inputs.run_all)
      run: ./run.sh INTEGRATIONS
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run APM_TRACING_E2E_OTEL scenario
      if: always() && steps.build.outcome == 'success' && (inputs.run_open_telemetry || inputs.run_all)
      run: ./run.sh APM_TRACING_E2E_OTEL
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
        DD_APPLICATION_KEY: ${{ secrets.DD_APPLICATION_KEY }}
        DD_APP_KEY: ${{ secrets.DD_APPLICATION_KEY }}
    - name: Run LIBRARY_CONF_CUSTOM_HEADER_TAGS scenario
      if: always() && steps.build.outcome == 'success' && inputs.run_all
      run: ./run.sh LIBRARY_CONF_CUSTOM_HEADER_TAGS
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run LIBRARY_CONF_CUSTOM_HEADER_TAGS_INVALID scenario
      if: always() && steps.build.outcome == 'success' && inputs.run_all
      run: ./run.sh LIBRARY_CONF_CUSTOM_HEADER_TAGS_INVALID
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run REMOTE_CONFIG_MOCKED_BACKEND_ASM_FEATURES scenario
      if: always() && steps.build.outcome == 'success' && inputs.run_all
      run: ./run.sh REMOTE_CONFIG_MOCKED_BACKEND_ASM_FEATURES
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run REMOTE_CONFIG_MOCKED_BACKEND_LIVE_DEBUGGING scenario
      if: always() && steps.build.outcome == 'success' && inputs.run_all
      run: ./run.sh REMOTE_CONFIG_MOCKED_BACKEND_LIVE_DEBUGGING
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run REMOTE_CONFIG_MOCKED_BACKEND_ASM_DD scenario
      if: always() && steps.build.outcome == 'success' && inputs.run_all
      run: ./run.sh REMOTE_CONFIG_MOCKED_BACKEND_ASM_DD
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run REMOTE_CONFIG_MOCKED_BACKEND_ASM_FEATURES_NOCACHE scenario
      if: always() && steps.build.outcome == 'success' && inputs.run_all
      run: ./run.sh REMOTE_CONFIG_MOCKED_BACKEND_ASM_FEATURES_NOCACHE
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run REMOTE_CONFIG_MOCKED_BACKEND_LIVE_DEBUGGING_NOCACHE scenario
      if: always() && steps.build.outcome == 'success' && inputs.run_all
      run: ./run.sh REMOTE_CONFIG_MOCKED_BACKEND_LIVE_DEBUGGING_NOCACHE
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run REMOTE_CONFIG_MOCKED_BACKEND_ASM_DD_NOCACHE scenario
      if: always() && steps.build.outcome == 'success' && inputs.run_all
      run: ./run.sh REMOTE_CONFIG_MOCKED_BACKEND_ASM_DD_NOCACHE
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run APPSEC_MISSING_RULES scenario
      if: always() && steps.build.outcome == 'success' && (inputs.run_appsec || inputs.run_all)
      run: ./run.sh APPSEC_MISSING_RULES
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run APPSEC_CUSTOM_RULES scenario
      if: always() && steps.build.outcome == 'success' && (inputs.run_appsec || inputs.run_all)
      run: ./run.sh APPSEC_CUSTOM_RULES
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run APPSEC_CORRUPTED_RULES scenario
      if: always() && steps.build.outcome == 'success' && (inputs.run_appsec || inputs.run_all)
      run: ./run.sh APPSEC_CORRUPTED_RULES
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run APPSEC_RULES_MONITORING_WITH_ERRORS scenario
      if: always() && steps.build.outcome == 'success' && (inputs.run_appsec || inputs.run_all)
      run: ./run.sh APPSEC_RULES_MONITORING_WITH_ERRORS
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run APPSEC_BLOCKING scenario
      if: always() && steps.build.outcome == 'success' && (inputs.run_appsec || inputs.run_all)
      run: ./run.sh APPSEC_BLOCKING
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run APPSEC_DISABLED scenario
      if: always() && steps.build.outcome == 'success' && (inputs.run_appsec || inputs.run_all)
      run: ./run.sh APPSEC_DISABLED
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run APPSEC_LOW_WAF_TIMEOUT scenario
      if: always() && steps.build.outcome == 'success' && (inputs.run_appsec || inputs.run_all)
      run: ./run.sh APPSEC_LOW_WAF_TIMEOUT
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run APPSEC_CUSTOM_OBFUSCATION scenario
      if: always() && steps.build.outcome == 'success' && (inputs.run_appsec || inputs.run_all)
      run: ./run.sh APPSEC_CUSTOM_OBFUSCATION
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run APPSEC_RATE_LIMITER scenario
      if: always() && steps.build.outcome == 'success' && (inputs.run_appsec || inputs.run_all)
      run: ./run.sh APPSEC_RATE_LIMITER
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run APPSEC_BLOCKING_FULL_DENYLIST scenario
      if: always() && steps.build.outcome == 'success' && (inputs.run_appsec || inputs.run_all)
      run: ./run.sh APPSEC_BLOCKING_FULL_DENYLIST
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run APPSEC_REQUEST_BLOCKING scenario
      if: always() && steps.build.outcome == 'success' && (inputs.run_appsec || inputs.run_all)
      run: ./run.sh APPSEC_REQUEST_BLOCKING
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run APPSEC_RUNTIME_ACTIVATION scenario
      if: always() && steps.build.outcome == 'success' && (inputs.run_appsec || inputs.run_all)
      run: ./run.sh APPSEC_RUNTIME_ACTIVATION
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run APPSEC_WAF_TELEMETRY scenario
      if: always() && steps.build.outcome == 'success' && (inputs.run_appsec || inputs.run_all)
      run: ./run.sh APPSEC_WAF_TELEMETRY
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run APPSEC_API_SECURITY scenario
      if: always() && steps.build.outcome == 'success' && (inputs.run_appsec || inputs.run_all)
      run: ./run.sh APPSEC_API_SECURITY
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run APPSEC_API_SECURITY_RC scenario
      if: always() && steps.build.outcome == 'success' && (inputs.run_appsec || inputs.run_all)
      run: ./run.sh APPSEC_API_SECURITY_RC
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run APPSEC_API_SECURITY_NO_RESPONSE_BODY scenario
      if: always() && steps.build.outcome == 'success' && (inputs.run_appsec || inputs.run_all)
      run: ./run.sh APPSEC_API_SECURITY_NO_RESPONSE_BODY
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run APPSEC_API_SECURITY_WITH_SAMPLING scenario
      if: always() && steps.build.outcome == 'success' && (inputs.run_appsec || inputs.run_all)
      run: |
        ./run.sh APPSEC_API_SECURITY_WITH_SAMPLING
        cat ./logs_appsec_api_security_with_sampling/tests.log 2>/dev/null | grep "API SECURITY" || true
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run APPSEC_AUTO_EVENTS_EXTENDED scenario
      if: always() && steps.build.outcome == 'success' && (inputs.run_appsec || inputs.run_all)
      run: ./run.sh APPSEC_AUTO_EVENTS_EXTENDED
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run SAMPLING scenario
      if: always() && steps.build.outcome == 'success' && (inputs.run_sampling || inputs.run_all)
      run: ./run.sh SAMPLING
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}

    - name: Run TELEMETRY_APP_STARTED_PRODUCTS_DISABLED scenario
      if: always() && steps.build.outcome == 'success' && inputs.run_all
      run: ./run.sh TELEMETRY_APP_STARTED_PRODUCTS_DISABLED
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run TELEMETRY_LOG_GENERATION_DISABLED scenario
      if: always() && steps.build.outcome == 'success' && inputs.run_all
      run: ./run.sh TELEMETRY_LOG_GENERATION_DISABLED
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run TELEMETRY_METRIC_GENERATION_DISABLED scenario
      if: always() && steps.build.outcome == 'success' && inputs.run_all
      run: ./run.sh TELEMETRY_METRIC_GENERATION_DISABLED
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run TELEMETRY_METRIC_GENERATION_ENABLED scenario
      if: always() && steps.build.outcome == 'success' && inputs.run_all
      run: ./run.sh TELEMETRY_METRIC_GENERATION_ENABLED
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}

    - name: Run TELEMETRY_DEPENDENCY_LOADED_TEST_FOR_DEPENDENCY_COLLECTION_DISABLED scenario
      if: always() && steps.build.outcome == 'success' && inputs.run_all
      run: ./run.sh TELEMETRY_DEPENDENCY_LOADED_TEST_FOR_DEPENDENCY_COLLECTION_DISABLED
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run DEBUGGER_PROBES_STATUS scenario
      if: always() && steps.build.outcome == 'success' && (inputs.run_debugger || inputs.run_all)
      run: ./run.sh DEBUGGER_PROBES_STATUS
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run DEBUGGER_METHOD_PROBES_SNAPSHOT scenario
      if: always() && steps.build.outcome == 'success' && (inputs.run_debugger || inputs.run_all)
      run: ./run.sh DEBUGGER_METHOD_PROBES_SNAPSHOT
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run DEBUGGER_LINE_PROBES_SNAPSHOT scenario
      if: always() && steps.build.outcome == 'success' && (inputs.run_debugger || inputs.run_all)
      run: ./run.sh DEBUGGER_LINE_PROBES_SNAPSHOT
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run DEBUGGER_MIX_LOG_PROBE scenario
      if: always() && steps.build.outcome == 'success' && (inputs.run_debugger || inputs.run_all)
      run: ./run.sh DEBUGGER_MIX_LOG_PROBE
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
    - name: Run DEBUGGER_PII_REDACTION scenario
      if: always() && steps.build.outcome == 'success' && (inputs.run_debugger || inputs.run_all)
      run: ./run.sh DEBUGGER_PII_REDACTION
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}

    - name: Run all scenarios in replay mode
      if: always() && steps.build.outcome == 'success'
      run: utils/scripts/replay_scenarios.sh
      env:
        DD_API_KEY: ${{ secrets.DD_API_KEY }}
        DD_APPLICATION_KEY: ${{ secrets.DD_APPLICATION_KEY }}
        DD_APP_KEY: ${{ secrets.DD_APPLICATION_KEY }}

    - name: Compress logs
      id: compress_logs
      if: always() && steps.build.outcome == 'success'
      run: tar -czvf artifact.tar.gz $(ls | grep logs)
    - name: Upload artifact
      if: always() && steps.compress_logs.outcome == 'success'
      uses: actions/upload-artifact@v4
      with:
        name: logs_endtoend_${{ inputs.library }}_${{ matrix.weblog }}_${{ matrix.version }}
        path: artifact.tar.gz

    - name: Push results to Feature Parity Dashboard
      if: ${{ matrix.version == 'dev' && inputs.push_to_feature_parity_dashbaord && false }} # disabled for now
      run: |
        for folder in logs*/ ; do
          curl -X POST ${FP_IMPORT_URL} \
            --fail
            --header "Content-Type: application/json" \
            --header "FP_API_KEY: ${FP_API_KEY}" \
            --data "@./${folder}feature_parity.json" \
            --include
        done
      env:
        FP_API_KEY: ${{ secrets.FP_API_KEY }}
        FP_IMPORT_URL: ${{ secrets.FP_IMPORT_URL }}

    - name: Print fancy log report
      if: ${{ always() }}
      run: python utils/scripts/markdown_logs.py >> $GITHUB_STEP_SUMMARY
